---
title: "Simulations"
author: "Jorge Garcia Condado"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    self_contained: true
knit: (
  function(inputFile, encoding) { 
    rmarkdown::render('Simulations.Rmd',
      output_file = paste('~/Documents/Projects/AgingBrainStudy/results/Simulations', '.html', sep=''))
      })
---

```{r, include = FALSE, warning = FALSE, message = FALSE}
library(ggplot2); library(ggpubr); library(MASS); library(dplyr);library(nlme);library(sjPlot);library(longpower);library(parallel);


knitr::opts_chunk$set(echo = FALSE, message = FALSE)
theme_set(theme_bw())
```


# Brain Age power analysis

```{r}

# Load HABS data
data_habs <- read.csv('~/Documents/Projects/AgingBrainStudy/data/HABS/raw/longitudinal_PACC.csv')

# Load delta
deltas_habs <- read.csv('~/Documents/Projects/AgingBrainStudy/analysis/HABS/structural_brainage/ageml/model_age/predicted_age.csv')
data_habs <- merge(data_habs, deltas_habs, by = 'SubjIDshort', all.x = TRUE)

# Create new variable delta_group where is 0 below younger and older above zero
data_habs <- data_habs %>% 
  mutate(delta_group = ifelse(delta_all < 0, 'younger', 'older'))

# Add factors
factors_habs <- read.csv('~/Documents/Projects/AgingBrainStudy/data/HABS/processed/factors.csv')
factors_habs <- factors_habs %>% 
  select(SubjIDshort, p_tau217, p_tau217_ratio, PIB_FS_SUVR_FLR, PIB_FS_SUVR_Group, tau_composite)
data_habs <- merge(data_habs, factors_habs, by = 'SubjIDshort', all.x = TRUE)

# Ensure correct typing
data_habs$NP_SessionDate <- as.Date(data_habs$NP_SessionDate)
data_habs <- data_habs %>%
  mutate(across(c(E4_Status, PIB_FS_SUVR_Group), as.factor))

# Filter subjects with less than 2 data points
data_habs <- data_habs %>% 
  arrange(SubjIDshort, NP_SessionDate) %>%  
  group_by(SubjIDshort) %>% 
  add_tally(name = 'nTimePoints', wt = !is.na(NP_PACC_PACC5)) %>%
  filter(nTimePoints >= 2) %>%
  mutate(time_from_bl = as.numeric(NP_SessionDate - min(NP_SessionDate, na.rm = TRUE))/365.25)  %>%
  mutate(PACC_change_bl = NP_PACC_PACC5 - NP_PACC_PACC5[1])

# Create baseline data set
baseline_habs <- data_habs %>% 
  group_by(SubjIDshort) %>%
  arrange(NP_SessionDate) %>%
  slice(1) %>% #extract first rows - can also use slice_head() for first and slice_tail() for last
  rename(BL_Age = NP_Age) %>%
  rename(BL_PACC = NP_PACC_PACC5) %>%
  ungroup()

# Add age at baseline of subjects
data_habs <- data_habs %>%
  left_join(baseline_habs %>% select(SubjIDshort, BL_Age, BL_PACC), by = 'SubjIDshort')

# Ensure factors
data_habs <- data_habs %>%
  mutate_at(vars(SEX, E4_Status), factor)
```

Linear mixed effects model

```{r}
# remove those with delta lower than 2
data_habs <- data_habs %>% filter(delta_all > 0)
lmm_results_habs_pacc <- lme(NP_PACC_PACC5 ~ time_from_bl, data = data_habs, random = ~time_from_bl|SubjIDshort, na.action = 'na.omit', method = c('ML'));
#lmm_results_habs_pacc <- lme(NP_PACC_PACC5 ~ time_from_bl*delta_all + time_from_bl*BL_Age + time_from_bl*E4_Status + time_from_bl*PIB_FS_SUVR_Group + time_from_bl*SEX + time_from_bl*YrsEd, data = data_habs, random = ~time_from_bl|SubjIDshort, na.action = 'na.omit', method = c('ML'));
summary(lmm_results_habs_pacc)
plot_model(lmm_results_habs_pacc, type = c('pred'), terms = c('time_from_bl'))
```
```{r}
# Variables of model

# Trial
effect = 0.27
time_points = seq(0, 4, 4)
sig.level = 0.05
power = 0.9

# LMM metrics
vc_matrix <- VarCorr(lmm_results_habs_pacc)
vc_numeric <- as.numeric(vc_matrix)
random_intercept_var = vc_numeric[1]
random_slope_var = vc_numeric[2]
random_residual_var = vc_numeric[3]
corr_intercept_slope = vc_numeric[8]
slope = summary(lmm_results_habs_pacc)$tTable["time_from_bl", 1]
intercept = summary(lmm_results_habs_pacc)$tTable["(Intercept)", 1]

# Delta expected
delta_expected = slope * effect
```

## Formulas

```{r}
lmmpower(lmm_results_habs_pacc, delta = delta_expected, t = time_points, sig.level = sig.level, power = power)
lmmpower(object=lmm_results_habs_pacc, sig2.i = random_intercept_var, sig2.s = random_slope_var, sig2.e = random_residual_var, cor.s.i = corr_intercept_slope, pct.change = effect, t = time_points, sig.level = sig.level, power = power)
```

```{r}
# Using formula directly

# Generate time points
t <- time_points
n <- length(t)

# Covariance function
cov.t <- function(t1, t2, sig2.i, sig2.s, cov.s.i){
    sig2.i + t1*t2*sig2.s + (t1+t2)*cov.s.i 
}

# Create covariance matrix R
# Use the variance components from your model
R <- outer(t, t, function(x,y){
    cov.t(x, y, 
          sig2.i = random_intercept_var, 
          sig2.s = random_slope_var, 
          cov.s.i = corr_intercept_slope)
})

# Add residual variance to the diagonal
R <- R + diag(random_residual_var, n, n)

# Perform power calculation
power_result <- diggle.linear.power(
    d = delta_expected,     # Treatment effect
    t = t,                    # Time points
    R = R,                    # Covariance matrix
    sig.level = 0.05,         # Significance level
    power = 0.90              # Desired power
)

# Print results
print(power_result)
```

## Simulations

### Data simulations

```{r}
simulate_longitudinal_data <- function(
  n,                      # Number of subjects
  t,                      # Vector of time points
  intercept,              # Fixed intercept
  slope,                  # Fixed slope
  random_intercept_var,   # Variance of random intercept
  random_slope_var,       # Variance of random slope
  corr_intercept_slope,   # Correlation between intercept and slope
  random_residual_var,    # Variance of residual error
  effect                  # Treatment effect
) {
  
  # Random effects simulation
  cov_intercept_slope <- corr_intercept_slope * sqrt(random_intercept_var) * sqrt(random_slope_var)
  sigma_matrix <- matrix(
    c(random_intercept_var, cov_intercept_slope, cov_intercept_slope, random_slope_var),
    nrow = 2, byrow = TRUE
  )
  randeff <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma_matrix)
  
  # Assignation of treatment
  data <- data.frame(
    id = 1:n,
    randint = randeff[, 1],
    randsl = randeff[, 2],
    txt = rep(c(0, 1), length.out=n)
  )
  
  # Simulate observations dynamically
  for (i in seq_along(t)) {
    data[[paste0("obs", i)]] <- intercept + data$randint + 
      t[i] * (slope + data$randsl - slope * effect * data$txt) +
      rnorm(n, 0, random_residual_var)
  }
  
  # Convert to long format
  data_long <- data %>%
    tidyr::pivot_longer(
      cols = starts_with("obs"),
      names_to = "timepoint",
      values_to = "obs"
    ) %>%
    mutate(time = rep(t, times = n))
  
  # Sort by subject ID and time
  data_long <- data_long[order(data_long$id, data_long$time), ]
  
  return(data_long)
}
```


### Modelling
```{r}
# Specific trial metrics
numsubj = 100

# Simulate data
data_long <- simulate_longitudinal_data(
  n = numsubj,
  t = t,
  intercept = intercept,
  slope = slope,
  random_intercept_var = random_intercept_var,
  random_slope_var = random_slope_var,
  corr_intercept_slope = corr_intercept_slope,
  random_residual_var = random_residual_var,
  effect = effect
)

# Create lme model and show results
results <- lme(obs ~ time*txt, random = ~time|id, data = data_long, control=lmeControl(maxiter=100,opt="optim"))
summary(results)
plot_model(results, type = c('pred'), terms = c('time', 'txt'))
```

```{r}
# Calculate change from baseline
data_long_change <- data_long %>%
  group_by(id) %>%
  mutate(change = obs - obs[time == 0]) %>%
  filter(time == 4) %>%
  select(id, change, txt) 
# Plot distribution by txt
ggplot(data_long_change, aes(x = change, fill = factor(txt))) +
  geom_density(alpha = 0.5) +
  theme(legend.position = 'top')
# Run ptest only 
t.test(change ~ txt, data = data_long_change, alternative = 'less')
```

```{r}
# Create simulate trial function for slopes
simulate_trial_diff <- function(
  n,                      # Number subjects
  t,                    # Vector of time points
  intercept,            # Fixed intercept
  slope,                # Fixed slope
  random_intercept_var, # Variance of random intercept
  random_slope_var,     # Variance of random slope
  corr_intercept_slope, # Correlation between intercept and slope
  random_residual_var,  # Variance of residual error
  effect               # Treatment effect
) 
 {
  data_long <- simulate_longitudinal_data(
    n = n,
    t = t,
    intercept = intercept,
    slope = slope,
    random_intercept_var = random_intercept_var,
    random_slope_var = random_slope_var,
    corr_intercept_slope = corr_intercept_slope,
    random_residual_var = random_residual_var,
    effect = effect
  )
  
  # Calculate change from baseline
  data_long_change <- data_long %>%
    group_by(id) %>%
    mutate(change = obs - obs[time == 0]) %>%
    filter(time == t[length(t)]) %>%
    select(id, change, txt) 
  
  # Caculate t test where group 0 is less (worse pacc) than group 1
  tresult <- t.test(change ~ txt, data = data_long_change, alternative = 'less')
  pval <- tresult$p.value
  
  return(pval)
}
```


```{r}
# Create simulate trial function for slopes
simulate_trial <- function(
  n,                      # Number subjects
  t,                    # Vector of time points
  intercept,            # Fixed intercept
  slope,                # Fixed slope
  random_intercept_var, # Variance of random intercept
  random_slope_var,     # Variance of random slope
  corr_intercept_slope, # Correlation between intercept and slope
  random_residual_var,  # Variance of residual error
  effect               # Treatment effect
) 
 {
  data_long <- simulate_longitudinal_data(
    n = n,
    t = t,
    intercept = intercept,
    slope = slope,
    random_intercept_var = random_intercept_var,
    random_slope_var = random_slope_var,
    corr_intercept_slope = corr_intercept_slope,
    random_residual_var = random_residual_var,
    effect = effect
  )
  
  # Create lme model
  results <- lme(obs ~ time*txt, random = ~time|id, data = data_long, control=lmeControl(maxiter=100,opt="optim"))
  pval <- summary(results)$tTable["time:txt", 5]
  
  return(pval)
}
```

```{r}
# Simulate 
simulate_trial_diff(
  n = 1000,
  t = t,
  intercept = intercept,
  slope = slope,
  random_intercept_var = random_intercept_var,
  random_slope_var = random_slope_var,
  corr_intercept_slope = corr_intercept_slope,
  random_residual_var = random_residual_var,
  effect = effect
)

simulate_trial(
  n = 1000,
  t = t,
  intercept = intercept,
  slope = slope,
  random_intercept_var = random_intercept_var,
  random_slope_var = random_slope_var,
  corr_intercept_slope = corr_intercept_slope,
  random_residual_var = random_residual_var,
  effect = effect
)
```

### Power estiamtion
```{r}
estimate_power_parallel <- function(  
  # Basic simulation parameters
  t,                    # Vector of time points
  intercept,            # Fixed intercept
  slope,                # Fixed slope
  random_intercept_var, # Variance of random intercept
  random_slope_var,     # Variance of random slope
  corr_intercept_slope, # Correlation between intercept and slope
  random_residual_var,  # Variance of residual error
  effect,               # Treatment effect
  
  # simulation function
  simulation,
  
  # Sample size search parameters
  n,                      # Number subjects
  alpha = 0.05,           # Significance level
  reps = 100,             # Number of repetitions of trials
  cores = parallel::detectCores() - 1 # Number of cores to use
)
  {
  # Create the cluster
  cl <- makeCluster(cores)
  
  # Export necessary functions and objects to the cluster
  clusterExport(cl, c("simulate_trial", "simulate_trial_diff",
                      "simulate_longitudinal_data", 
                      "t", "intercept", "slope", "random_intercept_var", 
                      "random_slope_var", "corr_intercept_slope", 
                      "random_residual_var", "effect", "n"))
  
  # Ensure required libraries are loaded on each cluster node
  clusterEvalQ(cl, {
    library(MASS); library(dplyr);library(nlme);
  })
  
  # Use parLapply to compute p-values
  p_values <- parLapply(cl, 1:reps, function(i) {
    simulation(
      n = n,
      t = t,
      intercept = intercept,
      slope = slope,
      random_intercept_var = random_intercept_var,
      random_slope_var = random_slope_var,
      corr_intercept_slope = corr_intercept_slope,
      random_residual_var = random_residual_var,
      effect = effect
    )
  })
  
  # Stop the cluster
  stopCluster(cl)
  
  # Compute power
  power <- mean(unlist(p_values) < alpha)
  return(power)
}
```

```{r}
estimate_power_parallel(
  t = t,
  intercept = intercept,
  slope = slope,
  random_intercept_var = random_intercept_var,
  random_slope_var = random_slope_var,
  corr_intercept_slope = corr_intercept_slope,
  random_residual_var = random_residual_var,
  effect = effect,
  simulation = simulate_trial,
  n = 2000,
  reps = 100,
)
```

```{r}
estimate_power_parallel(
  t = t,
  intercept = intercept,
  slope = slope,
  random_intercept_var = random_intercept_var,
  random_slope_var = random_slope_var,
  corr_intercept_slope = corr_intercept_slope,
  random_residual_var = random_residual_var,
  effect = effect,
  simulation = simulate_trial_diff,
  n = 2000,
  reps = 100,
)
```

### Estimating sample size

```{r}
find_sample_size_parallel <- function(  
  # Basic simulation parameters
  t,                    # Vector of time points
  intercept,            # Fixed intercept
  slope,                # Fixed slope
  random_intercept_var, # Variance of random intercept
  random_slope_var,     # Variance of random slope
  corr_intercept_slope, # Correlation between intercept and slope
  random_residual_var,  # Variance of residual error
  effect,               # Treatment effect
  
  # Simulation function
  simulation,
  
  # Sample size search parameters
  n_min = 1000,                      # Minimum sample size to try
  n_max = 4000,                      # Maximum sample size to try
  power_target = 0.9,                # Target power
  alpha = 0.05,                      # Significance level
  reps = 100,                        # Number of repetitions for each sample size
  nearest = 50,                      # Round sample sizes to nearest multiple
  cores = parallel::detectCores() - 1 # Number of cores to use
)
  {
  
  # Ensure n_min and n_max are multiples of `nearest`
  n_min <- ceiling(n_min / nearest) * nearest
  n_max <- floor(n_max / nearest) * nearest
  
  prev_n_mid <- -1  # Track previous n_mid to detect stagnation

  while (n_min < n_max) {
    n_mid <- round((n_min + n_max) / (2 * nearest)) * nearest  # Round to nearest multiple
    print(n_mid)

    # If n_mid is the same as the last iteration, break to avoid infinite loop
    if (n_mid == prev_n_mid) {
      break
    }
    prev_n_mid <- n_mid  # Update tracker

    power_est <- estimate_power_parallel(
      n = n_mid,
      reps = reps,
      alpha = alpha,
      cores = cores,
      t = t,
      intercept = intercept,
      slope = slope,
      random_intercept_var = random_intercept_var,
      random_slope_var = random_slope_var,
      corr_intercept_slope = corr_intercept_slope,
      random_residual_var = random_residual_var,
      effect = effect,
      simulation = simulation
    )      
    print(power_est)

    if (power_est >= power_target) {
      n_max <- n_mid  # Reduce upper bound
    } else {
      n_min <- min(n_mid + nearest, n_max)  # Ensure we donâ€™t exceed n_max
    }

    # Stop if the search range is smaller than `nearest`
    if ((n_max - n_min) < nearest) {
      break
    }
  }

  return(n_mid)
}
```

```{r}
# Run sample size calculation 10 times, store the results and plot mean and standard deviations
sample_sizes <- replicate(10, find_sample_size_parallel(
  n_min = 500,
  n_max = 3000,
  power_target = 0.9,
  reps = 100,
  alpha = 0.05,
  cores = 4,
  t = t,
  intercept = intercept,
  slope = slope,
  random_intercept_var = random_intercept_var,
  random_slope_var = random_slope_var,
  corr_intercept_slope = corr_intercept_slope,
  random_residual_var = random_residual_var,
  effect = effect,
  simulation = simulate_trial_diff
))
mean_sample_sizes <- mean(sample_sizes)
sd_sample_sizes <- sd(sample_sizes)
```

